{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Shapley import *\n",
    "from TextRank import *\n",
    "from utils import *\n",
    "\n",
    "# import libraries\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from operator import itemgetter\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of all nouns\n",
    "NOUNS = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    '''\n",
    "    The master class for our Document Summerization module.\n",
    "    Incorporates all features related to Document\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        self.sents = sent_tokenize(self.document)\n",
    "        self.sents_id = {self.sents[i]: i for i in range(len(self.sents))}\n",
    "        self.word_freq = FreqDist(clean(self.document))\n",
    "        self.graph = None\n",
    "        self.params = { 'thresh': 0.3\n",
    "            \n",
    "        }\n",
    "        \n",
    "                \n",
    "    def __str__(self):\n",
    "        return self.document\n",
    "    \n",
    "    \n",
    "    def statistical_sim(self, sent1, sent2):\n",
    "        '''\n",
    "        Statistical similarity between sentences\n",
    "        based on the cosine method\n",
    "        Returns: float (the cosine similarity b/w sent1 and sent2)\n",
    "        '''\n",
    "        sent_token1 = Counter(sent1)\n",
    "        sent_token2 = Counter(sent2)\n",
    "        \n",
    "        intxn = set(sent_token1) & set(sent_token2)\n",
    "        numerator = sum([sent_token1[x] * sent_token2[x] for x in intxn])\n",
    "        \n",
    "        mod1 = sum([sent_token1[x]**2 for x in sent_token1.keys()])\n",
    "        mod2 = sum([sent_token2[x]**2 for x in sent_token2.keys()])\n",
    "        denominator = sqrt(mod1)*sqrt(mod2)\n",
    "        \n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "\n",
    "        return float(numerator)/denominator\n",
    "    \n",
    "    \n",
    "    def semantic_sim(self, sent1, sent2):\n",
    "        '''\n",
    "        A semantic similarity score between two sentences\n",
    "        based on WordNet\n",
    "        Returns: float (the semantic similarity measure)\n",
    "        '''\n",
    "        score = 0\n",
    "        sent1 = [word for word in sent1 if word in NOUNS]\n",
    "        sent2 = [word for word in sent2 if word in NOUNS]\n",
    "        for t1 in sent1:\n",
    "            for t2 in sent2:\n",
    "                score += semantic_score(t1,t2)\n",
    "        try:\n",
    "            return score/(len(sent1 + sent2))  \n",
    "        except:\n",
    "            return 10000\n",
    "    \n",
    "    \n",
    "    def construct_graph(self):\n",
    "        '''\n",
    "        Constructs the word similarity graph\n",
    "        '''\n",
    "        length = len(self.sents)\n",
    "        connected, adj_mat = [], [[0 for i in range(length)] for j in range(length)]\n",
    "        for pair in combinations(self.sents, 2):\n",
    "            cpair = clean(pair[0]), clean(pair[1])\n",
    "            weight = self.statistical_sim(*cpair) + \\\n",
    "                     self.semantic_sim(*cpair)\n",
    "            connected.append((pair[0], pair[1], weight))\n",
    "        for sent1, sent2, weight in connected:\n",
    "            adj_mat[self.sents_id[sent1]][self.sents_id[sent2]] = weight\n",
    "            adj_mat[self.sents_id[sent2]][self.sents_id[sent1]] = weight\n",
    "        self.graph = draw_graph(connected, self.params['thresh'])\n",
    "        self.adj_mat = adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = '''\n",
    "burning tires tear gas and clashes with riot police the ugly scenes that come with workers strikes are all too familiar in france a country constantly trying to balance its culture of workers rights with a more efficient economy.\n",
    "such scenes are being played out across the country friday as unions have called for workers to step up protests that have for the past week crippled parts of france.\n",
    "employees of oil refineries nuclear power plants and some public transportation have left one in three gas stations dry forcing vehicles to search for well stocked stations and causing long lines at the pump.\n",
    "people are now hoarding gas worried that it may be some time until supply levels are back to normal.\n",
    "the workers are protesting a labor reform bill put forward by the government that will make it easier for companies to hire and fire employees.\n",
    "the governments argument is that the strict laws that make french workers among the best protected in the world leave companies in a difficult position where they cant take on new staff.\n",
    "french prime minister manuel valls told local media on thursday that he might be willing to modify some of the proposals giving hope to french people that the protests and fuel shortages may soon stop.\n",
    "but workers unions friday responded with a call to step up rallies and blockades demanding a complete withdrawal of the bill.\n",
    "we call for the continuation and intensification of protests a group of unions behind the protests said in a statement.\n",
    "the governments violent words its contempt for the social movement and its refusal to withdraw this bill reinforces our commitment it said.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node =   0\t score = 1.208138\n",
      "node =   6\t score = 1.199243\n",
      "node =   2\t score = 1.160498\n",
      "node =   1\t score = 1.125947\n",
      "node =   5\t score = 1.092980\n",
      "node =   4\t score = 1.049301\n",
      "node =   9\t score = 0.848015\n",
      "node =   7\t score = 0.837494\n",
      "node =   3\t score = 0.807110\n",
      "node =   8\t score = 0.669530\n"
     ]
    }
   ],
   "source": [
    "multi_doc = Document(doc)\n",
    "multi_doc.construct_graph()\n",
    "\n",
    "textrank_score = textrank_weighted(multi_doc.graph)\n",
    "for sents, score in textrank_score:\n",
    "    print \"node = %3d\\t score = %f\"%(multi_doc.sents_id[sents], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node =   0\t Shapley value = 1.100900\n",
      "node =   1\t Shapley value = 1.020000\n",
      "node =   2\t Shapley value = 0.930500\n",
      "node =   6\t Shapley value = 0.843900\n",
      "node =   4\t Shapley value = 0.825700\n",
      "node =   3\t Shapley value = 0.815300\n",
      "node =   5\t Shapley value = 0.799500\n",
      "node =   9\t Shapley value = 0.747000\n",
      "node =   7\t Shapley value = 0.739900\n",
      "node =   8\t Shapley value = 0.683700\n"
     ]
    }
   ],
   "source": [
    "shapley_graph = Graph(multi_doc.adj_mat)\n",
    "\n",
    "\n",
    "\n",
    "shapley_rank = shapley_graph.shapley(100, 100)\n",
    "\n",
    "for key, value in shapley_rank:\n",
    "    print \"node = %3d\\t Shapley value = %f\"%(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k sents Id by TextRank\n",
      "[0, 6, 2, 1, 5, 4, 9, 7, 3, 8]\n",
      "\n",
      "Top-k sentence Id by Shapley\n",
      "[0, 1, 2, 6, 4, 3, 5, 9, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "print 'Top-k sents Id by TextRank'\n",
    "print [multi_doc.sents_id[sents] for sents, score in textrank_score]\n",
    "\n",
    "print '\\nTop-k sentence Id by Shapley'\n",
    "print shapley_graph.top_k(len(multi_doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
